#include "CameraMath.h"
#include "DeviceInterface.h"
#include "MathGLM.h"
#include "MathTypeConversion.h"

#include "opencv2/opencv.hpp"

glm::mat4 computeGLMCameraTransformMatrix(const IVideoSourceInterface *videoSource)
{
    const glm::quat glm_quat= MikanQuatd_to_glm_dquat(videoSource->getCameraOffsetOrientation());
    const glm::vec3 glm_pos= MikanVector3d_to_glm_dvec3(videoSource->getCameraOffsetPosition());
    const glm::mat4 glm_camera_xform = glm_mat4_from_pose(glm_quat, glm_pos);

    return glm_camera_xform;
}

glm::mat4 computeGLMCameraViewMatrix(const glm::mat4& poseXform)
{
	// Convert the camera pose transform into a modelview matrix
	// This is adapted from glm::lookAt
	const glm::vec3 right = glm::vec3(poseXform[0]);
	const glm::vec3 up = glm::vec3(poseXform[1]);
	const glm::vec3 forward = glm::vec3(poseXform[2]);
	const glm::vec3 eye = glm::vec3(poseXform[3]);

	glm::mat4 modelView(1.f);
	modelView[0][0] = right.x;
	modelView[1][0] = right.y;
	modelView[2][0] = right.z;
	modelView[0][1] = up.x;
	modelView[1][1] = up.y;
	modelView[2][1] = up.z;
	modelView[0][2] = forward.x;
	modelView[1][2] = forward.y;
	modelView[2][2] = forward.z;
	modelView[3][0] = -glm::dot(right, eye);
	modelView[3][1] = -glm::dot(up, eye);
	modelView[3][2] = -glm::dot(forward, eye);

    return modelView;
}

void computeOpenCVCameraExtrinsicMatrix(const IVideoSourceInterface *videoSource,
                                               cv::Matx34f &out)
{
    // Extrinsic matrix is the inverse of the camera pose matrix
    const glm::mat4 glm_camera_xform = computeGLMCameraTransformMatrix(videoSource);
    const glm::mat4 glm_mat = glm::inverse(glm_camera_xform);

    out(0, 0) = glm_mat[0][0]; out(0, 1) = glm_mat[1][0]; out(0, 2) = glm_mat[2][0]; out(0, 3) = glm_mat[3][0];
    out(1, 0) = glm_mat[0][1]; out(1, 1) = glm_mat[1][1]; out(1, 2) = glm_mat[2][1]; out(1, 3) = glm_mat[3][1];
    out(2, 0) = glm_mat[0][2]; out(2, 1) = glm_mat[1][2]; out(2, 2) = glm_mat[2][2]; out(2, 3) = glm_mat[3][2];
}

bool computeOpenCVCameraRelativePatternTransform(
	const MikanMonoIntrinsics& intrinsics,
    const t_opencv_point2d_list& imagePoints,
    const t_opencv_point3d_list& objectPointsMM,
    cv::Quatd& outOrientation,
    cv::Vec3d& outPositionMM)
{
    // Bail if there isn't a corresponding world point for every image point
	if (imagePoints.size() != objectPointsMM.size() || imagePoints.size() == 0)
	{
		return false;
	}

	// Fetch the 3x3 GLM camera intrinsic matrix and store into a 3x3 openCV matrix
	const MikanMatrix3d& glmIntrinsicMatrix = intrinsics.camera_matrix;
	cv::Matx33d cvIntrinsicMatrix = MikanMatrix3d_to_cv_mat33d(glmIntrinsicMatrix);

	// Store the distortion parameters in a row vector with 8 values: [k1, k2, p1, p2, k3, k4, k5, k6]
	const MikanDistortionCoefficients& distortion_coeffs = intrinsics.distortion_coefficients;
	cv::Matx81d cvDistCoeffsColVector = Mikan_distortion_to_cv_vec8(distortion_coeffs);
	cv::Mat cvDistCoeffsRowVector;
	cv::transpose(cvDistCoeffsColVector, cvDistCoeffsRowVector);

	// Given an object model and the image points samples we could be able to compute 
	// a position and orientation of the mat relative to the camera
	cv::Mat rvec;
	cv::Mat tvecMM; // Mat position in millimeters
	if (!cv::solvePnP(
		objectPointsMM, imagePoints,
		cvIntrinsicMatrix, cvDistCoeffsRowVector,
		rvec, tvecMM))
	{
		return false;
	}

    outOrientation = cv::Quatd::createFromRvec(rvec);
    outPositionMM= tvecMM;

	return true;
}

void convertOpenCVCameraRelativePoseToGLMMat(
    const cv::Quatd& orientation,
    const cv::Vec3d& positionMM,
    glm::dmat4& outXform)
{
    // OpenCV camera relative locations in millimeters (by convention from object points passed into solvePnP), 
    // but the output GLM transform is in meters (by convention as defined by our renderer)
    cv::Vec3d tvec = positionMM * k_millimeters_to_meters;

	// Convert the OpenCV Rodrigues vector used to store orientation into a 3x3 rotation matrix
    cv::Matx33d rmat = orientation.toRotMat3x3();

	// Compose the openCV  rotation and translation together,
	// then convert OpenCV coordinates to OpenGL coordinates 
    // (x, y, z) -> (x, -y, -z)
	cv::Matx44d cv_RTMat = cv::Matx44d(
		rmat(0, 0), rmat(0, 1), rmat(0, 2), tvec(0),
		-rmat(1, 0), -rmat(1, 1), -rmat(1, 2), -tvec(1),
		-rmat(2, 0), -rmat(2, 1), -rmat(2, 2), -tvec(2),
		0.0F, 0.0F, 0.0F, 1.0F
	);

	// Convert OpenCV matrix to GLM matrix
    glm::mat4 RTMat;
	for (int row = 0; row < 4; ++row)
	{
		for (int col = 0; col < 4; ++col)
		{
			// GLM indexed by column first
			// OpenCV indexed by row first
            RTMat[col][row] = cv_RTMat(row, col);
		}
	}

	// Camera relative poses generated by cv::solvePnP have +Z pointed below the pattern.
	// Rotate by 180 degrees about +X to make +Z point above the pattern.
	glm::mat4 flipAboutX = glm::rotate(glm::mat4(1.f), glm::radians(180.0f), glm::vec3(1.0f, 0.0f, 0.0f));
	outXform = RTMat * flipAboutX;
}

void computeOpenCVCameraIntrinsicMatrix(const IVideoSourceInterface *videoSource,
                                        VideoFrameSection section,
                                        cv::Matx33d &intrinsicOut,
                                        cv::Matx81d &distortionOut)
{
    MikanVideoSourceIntrinsics tracker_intrinsics;
    videoSource->getCameraIntrinsics(tracker_intrinsics);

    MikanMatrix3d* camera_matrix= nullptr;
    MikanDistortionCoefficients* distortion_coefficients= nullptr;

    if (tracker_intrinsics.intrinsics_type == STEREO_CAMERA_INTRINSICS)
    {
        if (section == VideoFrameSection::Left)
        {
            camera_matrix= &tracker_intrinsics.intrinsics.stereo.left_camera_matrix;
            distortion_coefficients = &tracker_intrinsics.intrinsics.stereo.left_distortion_coefficients;
        }
        else if (section == VideoFrameSection::Right)
        {
            camera_matrix= &tracker_intrinsics.intrinsics.stereo.right_camera_matrix;
            distortion_coefficients = &tracker_intrinsics.intrinsics.stereo.right_distortion_coefficients;
        }
    }
    else if (tracker_intrinsics.intrinsics_type == MONO_CAMERA_INTRINSICS)
    {
        camera_matrix = &tracker_intrinsics.intrinsics.mono.camera_matrix;
        distortion_coefficients = &tracker_intrinsics.intrinsics.mono.distortion_coefficients;
    }

    if (camera_matrix != nullptr && distortion_coefficients != nullptr)
    {  
        intrinsicOut= MikanMatrix3d_to_cv_mat33d(*camera_matrix);
        distortionOut= Mikan_distortion_to_cv_vec8(*distortion_coefficients);
    }
}

void extractCameraIntrinsicMatrixParameters(const cv::Matx33f &intrinsic_matrix,
											float &out_focal_length_x,
											float &out_focal_length_y,
											float &out_principal_point_x,
											float &out_principal_point_y)
{
	out_focal_length_x= intrinsic_matrix(0, 0);
	out_focal_length_y= intrinsic_matrix(1, 1);
	out_principal_point_x= intrinsic_matrix(0, 2);
	out_principal_point_y= intrinsic_matrix(1, 2);
}

bool computeOpenCVCameraRectification(const IVideoSourceInterface *videoSource,
                                        VideoFrameSection section,
                                        cv::Matx33d &rotationOut,
                                        cv::Matx34d &projectionOut)
{
    MikanVideoSourceIntrinsics tracker_intrinsics;
    videoSource->getCameraIntrinsics(tracker_intrinsics);

    MikanMatrix3d* rectification_rotation= nullptr;
    MikanMatrix4x3d* rectification_projection= nullptr;

    if (tracker_intrinsics.intrinsics_type == STEREO_CAMERA_INTRINSICS)
    {
        if (section == VideoFrameSection::Left)
        {
            rectification_rotation= &tracker_intrinsics.intrinsics.stereo.left_rectification_rotation;
            rectification_projection= &tracker_intrinsics.intrinsics.stereo.left_rectification_projection;
        }
        else if (section == VideoFrameSection::Right)
        {
            rectification_rotation= &tracker_intrinsics.intrinsics.stereo.right_rectification_rotation;
            rectification_projection= &tracker_intrinsics.intrinsics.stereo.right_rectification_projection;
        }
    }

    if (rectification_rotation != nullptr && rectification_projection != nullptr)
    {
        rotationOut = MikanMatrix3d_to_cv_mat33d(*rectification_rotation);
        projectionOut = MikanMatrix4x3d_to_cv_mat34d(*rectification_projection);

        return true;
    }
    else
    {
        return false;
    }
}